
@article{foerster_learning_2016,
	title = {Learning to Communicate with Deep Multi-Agent Reinforcement Learning},
	url = {http://arxiv.org/abs/1605.06676},
	abstract = {We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning ({RIAL}) and Differentiable Inter-Agent Learning ({DIAL}). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.},
	journaltitle = {{arXiv}:1605.06676 [cs]},
	author = {Foerster, Jakob N. and Assael, Yannis M. and de Freitas, Nando and Whiteson, Shimon},
	urldate = {2018-04-05},
	date = {2016-05-21},
	eprinttype = {arxiv},
	eprint = {1605.06676},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Computer Science - Multiagent Systems},
	file = {arXiv\:1605.06676 PDF:/home/nle6013/Zotero/storage/WPRN55GW/Foerster et al. - 2016 - Learning to Communicate with Deep Multi-Agent Rein.pdf:application/pdf;arXiv.org Snapshot:/home/nle6013/Zotero/storage/VFQ6BGD9/1605.html:text/html}
}

@article{panait_cooperative_2005,
	title = {Cooperative Multi-Agent Learning: The State of the Art},
	volume = {11},
	doi = {10.1007/s10458-005-2631-2},
	shorttitle = {Cooperative Multi-Agent Learning},
	abstract = {Cooperative multi-agent systems ({MAS}) are ones in which several agents attempt, through their interaction, to jointly solve tasks or to maximize utility. Due to the interactions among the agents, multi-agent problem complexity can rise rapidly with the number of agents or their behavioral sophistication. The challenge this presents to the task of programming solutions to {MAS} problems has spawned increasing interest in machine learning techniques to automate the search and optimization process. We provide a broad survey of the cooperative multi-agent learning literature. Previous surveys of this area have largely focused on issues common to specific subareas (for example, reinforcement learning, {RL} or robotics). In this survey we attempt to draw from multi-agent learning work in a spectrum of areas, including {RL}, evolutionary computation, game theory, complex systems, agent modeling, and robotics. We find that this broad view leads to a division of the work into two categories, each with its own special issues: applying a single learner to discover joint solutions to multi-agent problems (team learning), or using multiple simultaneous learners, often one per agent (concurrent learning). Additionally, we discuss direct and indirect communication in connection with learning, plus open issues in task decomposition, scalability, and adaptive dynamics. We conclude with a presentation of multi-agent learning problem domains, and a list of multi-agent learning resources.},
	pages = {387--434},
	journaltitle = {Autonomous Agents and Multi-Agent Systems},
	author = {Panait, Liviu and Luke, Sean},
	date = {2005-11-01},
	file = {Panait and Luke - 2005 - Cooperative Multi-Agent Learning The State of the.pdf:/home/nle6013/Zotero/storage/UM6IN6DT/Panait and Luke - 2005 - Cooperative Multi-Agent Learning The State of the.pdf:application/pdf}
}

@article{g._barto_1_2008,
	title = {1 Reinforcement Learning and its Relationship to Supervised Learning},
	abstract = {The modern study of approximate dynamic programming ({DP}) combines ideas from several research traditions. Among these is the field of Artificial Intelligence, whose earliest period focussed on creating artificial learning systems. Today, Machine Learning is an active branch of Artificial Intelligence (although it includes researchers from many other disciplines as well) devoted to continuing the development of artificial learning systems. Some of the problems studied in Machine Learning concern stochastic sequential decision processes, and some approaches to solving them are based on {DP}. These problems and algorithms fall under the general heading of reinforcement learning. In this chapter, we discuss stochastic sequential decision processes from the perspective of Machine Learning, focussing on reinforcement learning and its relationship to the more commmonly studied supervised learning problems. Machine Learning is the study of methods for constructing and improving software systems by analyzing examples of their behavior rather than by directly program- ming them. Machine Learning methods are appropriate in application settings where people are unable to provide precise specifications for desired program behavior, but where examples of desired behavior are available, or where it is possible to assign a measure of goodness to examples of behavior. Such situations include optical char- acter recognition, handwriting recognition, speech recognition, automated steering of automobiles, and robot control and navigation. A key property of tasks in which examples of desired behavior are available is that people can perform them quite easily, but people cannot articulate exactly how they perform them. Hence, people},
	journaltitle = {Handbook of Learning and Approximate Dynamic Programming},
	author = {G. {BARTO}, {ANDREW} and Dietterich, Thomas},
	date = {2008-12-03}
}

@article{yves_glorennec_reinforcement_2001,
	title = {Reinforcement Learning: an Overview},
	shorttitle = {Reinforcement Learning},
	abstract = {Reinforcement learning relies on the association between a goal and a scalar signal, interpreted as reward or punishment. The objective is not to reproduce some reference signal, but to progessively find, by trial and error, the policy maximizing the rewards. This paper presents the basis of reinforcement learning, and two model-free algorithms, Q-Learning and Fuzzy Q-Learning.},
	author = {Yves Glorennec, Pierre},
	date = {2001-04-06}
}

@article{sutton_reinforcement_1998,
	title = {Reinforcement Learning: An Introduction},
	volume = {9},
	doi = {10.1109/TNN.1998.712192},
	shorttitle = {Reinforcement Learning},
	abstract = {Of several responses made to the same situation, those which are accompanied or closely followed by satisfaction to the animal will, other things being equal, be more firmly connected with the situation, so that, when it recurs, they will be more likely to recur; those which are accompanied or closely followed by discomfort to the animal will, other things being equal, have their connections with that situation weakened, so that, when it recurs, they will be less likely to occur. The greater the satisfaction or discomfort, the greater the strengthening or weakening of the bond. (Thorndike, 1911) The idea of learning to make appropriate responses based on reinforcing events has its roots in early psychological theories such as Thorndike's "law of effect" (quoted above). Although several important contributions were made in the 1950s, 1960s and 1970s by illustrious luminaries such as Bellman, Minsky, Klopf and others (Farley and Clark, 1954; Bellman, 1957; Minsky, 1961; Samuel, 1963; Michie and Chambers, 1968; Grossberg, 1975; Klopf, 1982), the last two decades have wit- nessed perhaps the strongest advances in the mathematical foundations of reinforcement learning, in addition to several impressive demonstrations of the performance of reinforcement learning algo- rithms in real world tasks. The introductory book by Sutton and Barto, two of the most influential and recognized leaders in the field, is therefore both timely and welcome. The book is divided into three parts. In the first part, the authors introduce and elaborate on the es- sential characteristics of the reinforcement learning problem, namely, the problem of learning "poli- cies" or mappings from environmental states to actions so as to maximize the amount of "reward"},
	pages = {1054},
	journaltitle = {{IEEE} transactions on neural networks / a publication of the {IEEE} Neural Networks Council},
	author = {Sutton, Richard and G. Barto, Andrew},
	date = {1998-02-01}
}

@book{alpaydin_introduction_2010,
	edition = {2nd},
	title = {Introduction to Machine Learning},
	isbn = {978-0-262-01243-0},
	abstract = {The goal of machine learning is to program computers to use example data or past experience to solve a given problem. Many successful applications of machine learning exist already, including systems that analyze past sales data to predict customer behavior, optimize robot behavior so that a task can be completed using minimum resources, and extract knowledge from bioinformatics data. Introduction to Machine Learning is a comprehensive textbook on the subject, covering a broad array of topics not usually included in introductory machine learning texts. In order to present a unified treatment of machine learning problems and solutions, it discusses many methods from different fields, including statistics, pattern recognition, neural networks, artificial intelligence, signal processing, control, and data mining. All learning algorithms are explained so that the student can easily move from the equations in the book to a computer program. The text covers such topics as supervised learning, Bayesian decision theory, parametric methods, multivariate methods, multilayer perceptrons, local models, hidden Markov models, assessing and comparing classification algorithms, and reinforcement learning. New to the second edition are chapters on kernel machines, graphical models, and Bayesian estimation; expanded coverage of statistical tests in a chapter on design and analysis of machine learning experiments; case studies available on the Web (with downloadable results for instructors); and many additional exercises. All chapters have been revised and updated. Introduction to Machine Learning can be used by advanced undergraduates and graduate students who have completed courses in computer programming, probability, calculus, and linear algebra. It will also be of interest to engineers in the field who are concerned with the application of machine learning methods. Adaptive Computation and Machine Learning series},
	publisher = {The {MIT} Press},
	author = {Alpaydin, Ethem},
	date = {2010}
}

@article{puterman_markov_1995,
	title = {Markov Decision Processes : Discrete Stochastic Dynamic Programming / M.L. Puterman.},
	volume = {37},
	doi = {10.2307/1269932},
	shorttitle = {Markov Decision Processes},
	abstract = {Contenido: 1) Introducción; 2) Formulación del modelo; 3) Ejemplos; 4) Procesos de decisión de Markov de horizonte finito; 5) Fundamentos: modelos de horizonte finito; 6) Problemas descontados de la decisión de Markov; 7) Criterio previsto del total-recompensa; 8) Promedio de recompensa y criterios relacionados ; 9) Rango del criterio-multicadena y modelos de comunicación; 10) Descuento óptimo sensibles; 11) Modelos de tiempo-continuo.},
	journaltitle = {Technometrics},
	author = {Puterman, Martin},
	date = {1995-08-01}
}

@book{bellman_dynamic_2003,
	title = {Dynamic Programming},
	isbn = {978-0-486-42809-3},
	abstract = {An introduction to the mathematical theory of multistage decision processes, this text takes a \&quot;functional equation\&quot; approach to the discovery of optimum policies. Written by a leading developer of such policies, it presents a series of methods, uniqueness and existence theorems, and examples for solving the relevant equations. The text examines existence and uniqueness theorems, the optimal inventory equation, bottleneck problems in multistage production processes, a new formalism in the calculus of variation, strategies behind multistage games, and Markovian decision processes. Each chapter concludes with a problem set that Eric V. Denardo of Yale University, in his informative new introduction, calls \&quot;a rich lode of applications and research topics.\&quot; 1957 edition. 37 figures.},
	pagetotal = {388},
	publisher = {Courier Corporation},
	author = {Bellman, Richard Ernest},
	date = {2003},
	langid = {english},
	note = {Google-Books-{ID}: {fyVtp}3EMxasC},
	keywords = {Computers / Programming / General, Mathematics / Linear \& Nonlinear Programming}
}

@article{sutton_learning_1988,
	title = {Learning to predict by the methods of temporal differences},
	volume = {3},
	issn = {0885-6125, 1573-0565},
	url = {https://link.springer.com/article/10.1007/BF00115009},
	doi = {10.1007/BF00115009},
	abstract = {This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.},
	pages = {9--44},
	number = {1},
	journaltitle = {Machine Learning},
	shortjournal = {Mach Learn},
	author = {Sutton, Richard S.},
	urldate = {2018-07-05},
	date = {1988-08-01},
	langid = {english},
	file = {Full Text PDF:/home/nle6013/Zotero/storage/PP3X2DNH/Sutton - 1988 - Learning to predict by the methods of temporal dif.pdf:application/pdf;Snapshot:/home/nle6013/Zotero/storage/TR4INL3B/10.html:text/html}
}

@article{watkins_q-learning_1992,
	title = {Q-learning},
	volume = {8},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/BF00992698},
	doi = {10.1007/BF00992698},
	pages = {279--292},
	number = {3},
	journaltitle = {Machine Learning},
	author = {Watkins, Christopher J. C. H. and Dayan, Peter},
	urldate = {2018-07-05},
	date = {1992-05},
	langid = {english}
}

@incollection{busoniu_multi-agent_2010,
	title = {Multi-agent Reinforcement Learning: An Overview},
	volume = {310},
	shorttitle = {Multi-agent Reinforcement Learning},
	abstract = {Multi-agent systems can be used to address problems in a variety of domains, including robotics, distributed control, telecommunications,
and economics. The complexity of many tasks arising in these domains makes them difficult to solve with preprogrammed agent
behaviors. The agents must instead discover a solution on their own, using learning. A significant part of the research on
multi-agent learning concerns reinforcement learning techniques. This chapter reviews a representative selection of multi-agent
reinforcement learning algorithms for fully cooperative, fully competitive, and more general (neither cooperative nor competitive)
tasks. The benefits and challenges of multi-agent reinforcement learning are described. A central challenge in the field is
the formal statement of a multi-agent learning goal; this chapter reviews the learning goals proposed in the literature. The
problem domains where multi-agent reinforcement learning techniques have been applied are briefly discussed. Several multi-agent
reinforcement learning algorithms are applied to an illustrative example involving the coordinated transportation of an object
by two cooperative robots. In an outlook for the multi-agent reinforcement learning field, a set of important open issues
are identified, and promising research directions to address these issues are outlined.},
	pages = {183--221},
	booktitle = {Studies in Computational Intelligence},
	author = {Busoniu, Lucian and Babuska, Robert and De Schutter, Bart},
	date = {2010-07-17},
	doi = {10.1007/978-3-642-14435-6_7},
	file = {Full Text PDF:/home/nle6013/Zotero/storage/QVAY5RW8/Busoniu et al. - 2010 - Multi-agent Reinforcement Learning An Overview.pdf:application/pdf}
}

@article{busoniu_comprehensive_2008,
	title = {A Comprehensive Survey of Multiagent Reinforcement Learning},
	volume = {38},
	doi = {10.1109/TSMCC.2007.913919},
	abstract = {Multiagent systems are rapidly finding applications in a variety of domains, including robotics, distributed control, telecommunications, and economics. The complexity of many tasks arising in these domains makes them difficult to solve with preprogrammed agent behaviors. The agents must, instead, discover a solution on their own, using learning. A significant part of the research on multiagent learning concerns reinforcement learning techniques. This paper provides a comprehensive survey of multiagent reinforcement learning ({MARL}). A central issue in the field is the formal statement of the multiagent learning goal. Different viewpoints on this issue have led to the proposal of many different goals, among which two focal points can be distinguished: stability of the agents' learning dynamics, and adaptation to the changing behavior of the other agents. The {MARL} algorithms described in the literature aim---either explicitly or implicitly---at one of these two goals or at a combination of both, in a fully cooperative, fully competitive, or more general setting. A representative selection of these algorithms is discussed in detail in this paper, together with the specific issues that arise in each category. Additionally, the benefits and challenges of {MARL} are described along with some of the problem domains where the {MARL} techniques have been applied. Finally, an outlook for the field is provided.},
	pages = {156--172},
	journaltitle = {Systems, Man, and Cybernetics, Part C: Applications and Reviews, {IEEE} Transactions on},
	author = {Busoniu, Lucian and Babuska, Robert and De Schutter, Bart},
	date = {2008-04-01},
	file = {Full Text PDF:/home/nle6013/Zotero/storage/NBDXDGXI/Busoniu et al. - 2008 - A Comprehensive Survey of Multiagent Reinforcement.pdf:application/pdf}
}

@article{bloembergen_evolutionary_2015,
	title = {Evolutionary Dynamics of Multi-Agent Learning: A Survey},
	volume = {53},
	shorttitle = {Evolutionary Dynamics of Multi-Agent Learning},
	abstract = {The interaction of multiple autonomous agents gives rise to highly dynamic and nonde- terministic environments, contributing to the complexity in applications such as automated financial markets, smart grids, or robotics. Due to the sheer number of situations that may arise, it is not possible to foresee and program the optimal behaviour for all agents be- forehand. Consequently, it becomes essential for the success of the system that the agents can learn their optimal behaviour and adapt to new situations or circumstances. The past two decades have seen the emergence of reinforcement learning, both in single and multi- agent settings, as a strong, robust and adaptive learning paradigm. Progress has been substantial, and a wide range of algorithms are now available. An important challenge in the domain of multi-agent learning is to gain qualitative insights into the resulting system dynamics. In the past decade, tools and methods from evolutionary game theory have been successfully employed to study multi-agent learning dynamics formally in strategic interactions. This article surveys the dynamical models that have been derived for various multi-agent reinforcement learning algorithms, making it possible to study and compare them qualitatively. Furthermore, new learning algorithms that have been introduced us- ing these evolutionary game theoretic tools are reviewed. The evolutionary models can be used to study complex strategic interactions. Examples of such analysis are given for the domains of automated trading in stock markets and collision avoidance in multi-robot sys- tems. The paper provides a roadmap on the progress that has been achieved in analysing the evolutionary dynamics of multi-agent learning by highlighting the main results and accomplishments.},
	pages = {659--697},
	journaltitle = {Journal of Artificial Intelligence Research},
	author = {Bloembergen, Daan and Tuyls, Karl and Hennes, Daniel and Kaisers, Michael},
	date = {2015-08-01},
	file = {Full Text PDF:/home/nle6013/Zotero/storage/RGRFNBGC/Bloembergen et al. - 2015 - Evolutionary Dynamics of Multi-Agent Learning A S.pdf:application/pdf}
}

@article{l._leottau_decentralized_2017,
	title = {Decentralized Reinforcement Learning of Robot Behaviors},
	volume = {256},
	doi = {10.1016/j.artint.2017.12.001},
	abstract = {A multi-agent methodology is proposed for Decentralized Reinforcement Learning ({DRL}) of individual behaviors in problems where multi-dimensional action spaces are involved. When using this methodology, sub-tasks are learned in parallel
by individual agents working toward a common goal. In addition to proposing this methodology, three specific multi agent {DRL} approaches are considered: {DRL}-Independent, {DRL} Cooperative-Adaptive ({CA}), and {DRL}-Lenient. These approaches are validated and analyzed with an extensive empirical study using four different problems: 3D Mountain Car, {SCARA} Real-Time Trajectory Generation, Ball-Dribbling in humanoid soccer robotics, and Ball-Pushing using differential drive robots. The experimental validation provides evidence that {DRL} implementations show better performances and faster learning times than their centralized counterparts, while using less computational resources. {DRL}-Lenient and {DRL}-{CA} algorithms achieve the best final performances for the four tested problems, outperforming their {DRL}-Independent counterparts. Furthermore, the benefits of the {DRL}-Lenient and {DRL}-{CA} are more noticeable when the problem complexity increases and the centralized scheme becomes intractable given the available computational resources and training time.},
	journaltitle = {Artificial Intelligence},
	author = {L. Leottau, David and Ruiz-del-Solar, Javier and Babuska, Robert},
	date = {2017-12-01}
}

@article{bernstein_complexity_2013,
	title = {The Complexity of Decentralized Control of Markov Decision Processes},
	url = {http://arxiv.org/abs/1301.3836},
	abstract = {Planning for distributed agents with partial state information is considered from a decision- theoretic perspective. We describe generalizations of both the {MDP} and {POMDP} models that allow for decentralized control. For even a small number of agents, the finite-horizon problems corresponding to both of our models are complete for nondeterministic exponential time. These complexity results illustrate a fundamental difference between centralized and decentralized control of Markov processes. In contrast to the {MDP} and {POMDP} problems, the problems we consider provably do not admit polynomial-time algorithms and most likely require doubly exponential time to solve in the worst case. We have thus provided mathematical evidence corresponding to the intuition that decentralized planning problems cannot easily be reduced to centralized problems and solved exactly using established techniques.},
	journaltitle = {{arXiv}:1301.3836 [cs]},
	author = {Bernstein, Daniel S. and Zilberstein, Shlomo and Immerman, Neil},
	urldate = {2018-07-06},
	date = {2013-01-16},
	eprinttype = {arxiv},
	eprint = {1301.3836},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv\:1301.3836 PDF:/home/nle6013/Zotero/storage/ERMZCF8A/Bernstein et al. - 2013 - The Complexity of Decentralized Control of Markov .pdf:application/pdf;arXiv.org Snapshot:/home/nle6013/Zotero/storage/QHWV8Q4F/1301.html:text/html}
}

@inproceedings{t._j._spaan_decentralized_2006,
	title = {Decentralized planning under uncertainty for teams of communicating agents},
	volume = {2006},
	doi = {10.1145/1160633.1160678},
	abstract = {Decentralized partially observable Markov decision processes ({DEC}-{POMDPs}) form a general framework for planning for groups of cooperating agents that inhabit a stochastic and partially observable environment. Unfortunately, comput- ing optimal plans in a {DEC}-{POMDP} has been shown to be intractable ({NEXP}-complete), and approximate algorithms for specic subclasses have been proposed. Many of these algorithms rely on an (approximate) solution of the central- ized planning problem (i.e., treating the whole team as a single agent). We take a more decentralized approach, in which each agent only reasons over its own local state and some uncontrollable state features, which are shared by all team members. In contrast to other approaches, we model communication as an integral part of the agent's reasoning, in which the meaning of a message is directly encoded in the policy of the communicating agent. We explore iter- ative methods for approximately solving such models, and we conclude with some encouraging preliminary experimen- tal results.},
	pages = {249--256},
	booktitle = {{ACM} Transactions on Multimedia Computing, Communications, and Applications - {TOMCCAP}},
	author = {T. J. Spaan, Matthijs and J. Gordon, Geoffrey and Vlassis, Nikos},
	date = {2006-01-01}
}

@article{yang_multiagent_2004,
	title = {Multiagent Reinforcement Learning for Multi-Robot Systems: A Survey},
	shorttitle = {Multiagent Reinforcement Learning for Multi-Robot Systems},
	abstract = {Multiagent reinforcement learning for multirobot systems is a challenging issue in both robotics and artificial intelligence. With the ever increasing interests in theoretical researches and practical applications, currently there have been a lot of efforts towards providing some solutions to this challenge. However, there are still many difficulties in scaling up the multiagent reinforcement learning to multi-robot systems. The main objective of this paper is to provide a survey, though not completely on the multiagent reinforcement learning in multi-robot systems. After reviewing important advances in this field, some challenging problems and promising research directions are analyzed. A concluding remark is made from the perspectives of the authors.},
	author = {Yang, Erfu and Gu, Dongbing},
	date = {2004-06-27},
	file = {Full Text PDF:/home/nle6013/Zotero/storage/NZJ76TV9/Yang and Gu - 2004 - Multiagent Reinforcement Learning for Multi-Robot .pdf:application/pdf}
}

@inproceedings{melo_querypomdp:_2011,
	title = {{QueryPOMDP}: {POMDP}-based communication in multiagent systems},
	doi = {10.1007/978-3-642-34799-3_13},
	shorttitle = {{QueryPOMDP}},
	abstract = {Decentralized Partially Observable Markov Decision Processes (Dec-{POMDPs}) provide powerful modeling tools for multiagent decision-making in the face of uncertainty, but solving these models comes at a very high computational cost. Two avenues for side-stepping the computational burden can be identified: structured interactions between agents and intra-agent communication. In this paper, we focus on the interplay between these concepts, namely how sparse interactions impact the communication needs. A key insight is that in domains with local interactions the amount of communication necessary for successful joint behavior can be heavily reduced, due to the limited influence between agents. We exploit this insight by deriving local {POMDP} models that optimize each agent's communication behavior. Our experimental results show that our approach successfully exploits sparse interactions: we can effectively identify the situations in which it is beneficial to communicate, as well as trade off the cost of communication with overall task performance.},
	pages = {189--204},
	author = {Melo, Francisco and T. J. Spaan, Matthijs and J. Witwicki, Stefan},
	date = {2011-11-14}
}

@inproceedings{busoniu_decentralized_2007,
	title = {Decentralized Reinforcement Learning Control of a Robotic Manipulator},
	doi = {10.1109/ICARCV.2006.345351},
	abstract = {Multi-agent systems are rapidly finding applications in a variety of domains, including robotics, distributed control, telecommunications, etc. Learning approaches to multi-agent control, many of them based on reinforcement learning ({RL}), are investigated in complex domains such as teams of mobile robots. However, the application of decentralized {RL} to low-level control tasks is not as intensively studied. In this paper, we investigate centralized and decentralized {RL}, emphasizing the challenges and potential advantages of the latter. These are then illustrated on an example: learning to control a two-link rigid manipulator. Some open issues and future research directions in decentralized {RL} are outlined},
	pages = {1--6},
	author = {Busoniu, Lucian and De Schutter, Bart and Babuska, Robert},
	date = {2007-01-08},
	file = {Full Text PDF:/home/nle6013/Zotero/storage/AGXM5CPJ/Busoniu et al. - 2007 - Decentralized Reinforcement Learning Control of a .pdf:application/pdf}
}

@article{lowe_multi-agent_2017,
	title = {Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
	journaltitle = {Neural Information Processing Systems ({NIPS})},
	author = {Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
	date = {2017}
}

@article{mordatch_emergence_2017,
	title = {Emergence of Grounded Compositional Language in Multi-Agent Populations},
	url = {http://arxiv.org/abs/1703.04908},
	abstract = {By capturing statistical patterns in large corpora, machine learning has enabled significant advances in natural language processing, including in machine translation, question answering, and sentiment analysis. However, for agents to intelligently interact with humans, simply capturing the statistical patterns is insufficient. In this paper we investigate if, and how, grounded compositional language can emerge as a means to achieve goals in multi-agent populations. Towards this end, we propose a multi-agent learning environment and learning methods that bring about emergence of a basic compositional language. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. We also observe emergence of non-verbal communication such as pointing and guiding when language communication is unavailable.},
	journaltitle = {{arXiv}:1703.04908 [cs]},
	author = {Mordatch, Igor and Abbeel, Pieter},
	urldate = {2018-07-17},
	date = {2017-03-14},
	eprinttype = {arxiv},
	eprint = {1703.04908},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv\:1703.04908 PDF:/home/nle6013/Zotero/storage/VZEMGV7N/Mordatch and Abbeel - 2017 - Emergence of Grounded Compositional Language in Mu.pdf:application/pdf;arXiv.org Snapshot:/home/nle6013/Zotero/storage/DWG6K6W6/1703.html:text/html}
}

@article{havrylov_emergence_2017,
	title = {Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols},
	url = {http://arxiv.org/abs/1705.11192},
	shorttitle = {Emergence of Language with Multi-agent Games},
	abstract = {Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general {AI}. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. Unlike previous work, we require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). We compare a reinforcement learning approach and one using a differentiable relaxation (straight-through Gumbel-softmax estimator) and observe that the latter is much faster to converge and it results in more effective protocols. Interestingly, we also observe that the protocol we induce by optimizing the communication success exhibits a degree of compositionality and variability (i.e. the same information can be phrased in different ways), both properties characteristic of natural languages. As the ultimate goal is to ensure that communication is accomplished in natural language, we also perform experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.},
	journaltitle = {{arXiv}:1705.11192 [cs]},
	author = {Havrylov, Serhii and Titov, Ivan},
	urldate = {2018-07-17},
	date = {2017-05-31},
	eprinttype = {arxiv},
	eprint = {1705.11192},
	keywords = {Computer Science - Multiagent Systems, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv\:1705.11192 PDF:/home/nle6013/Zotero/storage/T5SUPDZE/Havrylov and Titov - 2017 - Emergence of Language with Multi-agent Games Lear.pdf:application/pdf;arXiv.org Snapshot:/home/nle6013/Zotero/storage/AUWCRKSV/1705.html:text/html}
}

@article{lazaridou_emergence_2018,
	title = {Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input},
	url = {https://openreview.net/forum?id=HJGv1Z-AW},
	abstract = {The ability of algorithms to evolve or learn (compositional) communication protocols has traditionally been studied in the language evolution literature through the use of emergent communication...},
	author = {Lazaridou, Angeliki and Hermann, Karl Moritz and Tuyls, Karl and Clark, Stephen},
	urldate = {2018-07-17},
	date = {2018-02-15},
	file = {Full Text PDF:/home/nle6013/Zotero/storage/F4LSTA3N/Lazaridou et al. - 2018 - Emergence of Linguistic Communication from Referen.pdf:application/pdf;Snapshot:/home/nle6013/Zotero/storage/5TI6DFFS/forum.html:text/html}
}

@inproceedings{kasai_learning_2008,
	title = {Learning of communication codes in multi-agent reinforcement learning problem},
	doi = {10.1109/SMCIA.2008.5045926},
	abstract = {Realization of cooperative behavior in multi-agent system is important for improving problem solving ability. Reinforcement learning is one of the learning methods for such cooperative behavior of agents. In this paper, we consider pursuit problem for multi-agent reinforcement learning with communication between the agents. In our study, the agents obtain communication codes through learning. Here, the codes are rules for communicating appropriate information under various situations. We call the learning of communication codes signal learning. The signal is expressed by bit sequence, and its length is set to be variable. We carried out experiment for performance comparison with varying the signal length from 0 to 4 bits. As a result, it has been shown that, in learning precision, the case of 1 bit or more bits communication outperformed the case of no communication. It also has been shown that 4 bits communication produced the best result among the five cases, while learning with longer signals required much more iterations.},
	eventtitle = {2008 {IEEE} Conference on Soft Computing in Industrial Applications},
	pages = {1--6},
	booktitle = {2008 {IEEE} Conference on Soft Computing in Industrial Applications},
	author = {Kasai, T. and Tenmoto, H. and Kamiya, A.},
	date = {2008-06},
	keywords = {agents communication, appropriate information communication, bit sequence, codes, communication code learning, Communication industry, Computer applications, Computer industry, cooperative behavior, Educational institutions, Electronics industry, learning (artificial intelligence), Learning systems, multi-agent systems, multiagent reinforcement learning problem, Multiagent systems, Problem-solving, reinforcement learning, signal learning, Systems engineering and theory, Telephony},
	file = {IEEE Xplore Abstract Record:/home/nle6013/Zotero/storage/NBF8L3NC/5045926.html:text/html}
}

@article{kottur_natural_2017,
	title = {Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog},
	url = {http://arxiv.org/abs/1706.08502},
	abstract = {A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, all learned without any human supervision! In this paper, using a Task and Tell reference game between two agents as a testbed, we present a sequence of 'negative' results culminating in a 'positive' one -- showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge 'naturally', despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.},
	journaltitle = {{arXiv}:1706.08502 [cs]},
	author = {Kottur, Satwik and Moura, José M. F. and Lee, Stefan and Batra, Dhruv},
	urldate = {2018-07-17},
	date = {2017-06-26},
	eprinttype = {arxiv},
	eprint = {1706.08502},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1706.08502 PDF:/home/nle6013/Zotero/storage/FVS4GB8W/Kottur et al. - 2017 - Natural Language Does Not Emerge 'Naturally' in Mu.pdf:application/pdf;arXiv.org Snapshot:/home/nle6013/Zotero/storage/FMAVN53H/1706.html:text/html}
}

@article{lazaridou_multi-agent_2016,
	title = {Multi-Agent Cooperation and the Emergence of (Natural) Language},
	url = {http://arxiv.org/abs/1612.07182},
	abstract = {The current mainstream approach to train natural language systems is to expose them to large amounts of text. This passive learning is problematic if we are interested in developing interactive machines, such as conversational agents. We propose a framework for language learning that relies on multi-agent communication. We study this learning in the context of referential games. In these games, a sender and a receiver see a pair of images. The sender is told one of them is the target and is allowed to send a message from a fixed, arbitrary vocabulary to the receiver. The receiver must rely on this message to identify the target. Thus, the agents develop their own language interactively out of the need to communicate. We show that two networks with simple configurations are able to learn to coordinate in the referential game. We further explore how to make changes to the game environment to cause the "word meanings" induced in the game to better reflect intuitive semantic properties of the images. In addition, we present a simple strategy for grounding the agents' code into natural language. Both of these are necessary steps towards developing machines that are able to communicate with humans productively.},
	journaltitle = {{arXiv}:1612.07182 [cs]},
	author = {Lazaridou, Angeliki and Peysakhovich, Alexander and Baroni, Marco},
	urldate = {2018-07-17},
	date = {2016-12-21},
	eprinttype = {arxiv},
	eprint = {1612.07182},
	keywords = {Computer Science - Multiagent Systems, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computer Science and Game Theory},
	file = {arXiv\:1612.07182 PDF:/home/nle6013/Zotero/storage/CG8K3DI9/Lazaridou et al. - 2016 - Multi-Agent Cooperation and the Emergence of (Natu.pdf:application/pdf;arXiv.org Snapshot:/home/nle6013/Zotero/storage/AX85NGMD/1612.html:text/html}
}

@incollection{resnick_grounding_1991,
	title = {Grounding in Communication},
	pages = {13--1991},
	booktitle = {Perspectives on Socially Shared Cognition},
	publisher = {American Psychological Association},
	author = {Clark, Herbert H. and Brennan, Susan E.},
	editor = {Resnick, Lauren and B, Levine and John, M. and Teasley, Stephanie and {D.}},
	date = {1991}
}

@article{horvitz_grounding_2000,
	title = {Grounding Criterion: Toward a Formal Theory of Grounding},
	shorttitle = {Grounding Criterion},
	abstract = {In a conversation, participants establish and maintain their mutual belief that their utterances have been understood well enough for current purposes – a process that has been referred to as grounding. In order to make a contribution to the conversation, participants typically do more than just produce the right utterance at the right time; they coordinate the presentation and acceptance of their utterances until they have reached a sufficient level of mutual understanding to move on, a level defined by the grounding criterion . Recent interest in employing grounding for use in collaborative dialog systems has highlighted difficulties in rendering hitherto qualitative intuitions about grounding into formal terms. In this paper, we propose a formalization of grounding based on decision theory that captures key intuitions about the contribution model while providing an explicit method for determining the grounding criterion. We illustrate the formalization by reviewing interactions between a user and a prototype spoken dialog system called the Bayesian Receptionist .},
	author = {Horvitz, Eric and Paek, Tim},
	date = {2000-01-01}
}

@article{clark_contributing_nodate,
	title = {Contributing to Discourse},
	volume = {13},
	rights = {© 1989 Cognitive Science Society, Inc.},
	issn = {1551-6709},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1302_7},
	doi = {10.1207/s15516709cog1302_7},
	abstract = {For people to contribute to discourse, they must do more than utter the right sentence at the right time. The basic requirement is that they add to their common ground in an orderly way. To do this, we argue, they try to establish for each utterance the mutual belief that the addressees have understood what the speaker meant well enough for current purposes. This is accomplished by the collective actions of the current contributor and his or her partners, and these result in units of conversation called contributions. We present a model of contributions and show how it accounts for a variety of features of everyday conversations.},
	pages = {259--294},
	number = {2},
	journaltitle = {Cognitive Science},
	author = {Clark, Herbert H. and Schaefer, Edward F.},
	urldate = {2018-07-27},
	langid = {english},
	file = {Full Text PDF:/home/nle6013/Zotero/storage/DVDV5AU7/Clark and Schaefer - Contributing to Discourse.pdf:application/pdf;Snapshot:/home/nle6013/Zotero/storage/JJT3JVUU/s15516709cog1302_7.html:text/html}
}

@inproceedings{koschmann_reconsidering_2003,
	title = {Reconsidering Common Ground: Examining Clark's Contribution Theory},
	shorttitle = {Reconsidering Common Ground},
	abstract = {Abstract. The constructs of \&quot;common ground \&quot; and \&quot;grounding \&quot; are frequently invoked in the {CSCW} literature as a mechanism by which participants engaged in joint activity coordinate their respective understandings of matters at hand. These constructs arise from a model of conversation developed by Herbert Clark and sometimes referred to as \&quot;contribution theory. \&quot; We describe here the basic features of this theory and attempt to apply it in analyzing a fragment of enacted interaction. The interaction was recorded during an abdominal surgery performed with the aid of an endoscopic camera. We encountered difficulties, however, in applying contribution theory as an analytic framework within this concrete setting. We found further that the notion of common ground represents a confusing metaphor rather than a useful explanatory mechanism. We conclude with a suggestion that researchers in the future seek ways of constructing descriptions of joint activity that do not rely on the troublesome notions of grounding and common ground. Such words insert a name in place of a problem, and let it go at that; they pull out no plums, and only say, \&quot;What a big boy am I!\&quot; Dewey and Bentley (1991)},
	booktitle = {In},
	publisher = {Kluwer Academic Publishing},
	author = {Koschmann, Timothy and Lebaron, Curtis D.},
	date = {2003},
	file = {Citeseer - Full Text PDF:/home/nle6013/Zotero/storage/3P8YCQCK/Koschmann and Lebaron - 2003 - Reconsidering Common Ground Examining Clark's Con.pdf:application/pdf;Citeseer - Snapshot:/home/nle6013/Zotero/storage/G3YJMR7W/summary.html:text/html}
}

@article{mnih_playing_2013,
	title = {Playing Atari with Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1312.5602},
	abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	journaltitle = {{arXiv}:1312.5602 [cs]},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	urldate = {2018-07-30},
	date = {2013-12-19},
	eprinttype = {arxiv},
	eprint = {1312.5602},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1312.5602 PDF:/home/nle6013/Zotero/storage/UZFUJL54/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf:application/pdf;arXiv.org Snapshot:/home/nle6013/Zotero/storage/ERFEGXMT/1312.html:text/html}
}

@article{silver_mastering_2016,
	title = {Mastering the game of Go with deep neural networks and tree search},
	volume = {529},
	doi = {10.1038/nature16961},
	abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program {AlphaGo} achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
	pages = {484--489},
	journaltitle = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Christopher and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	date = {2016-01-27}
}

@book{altman_constrained_1999,
	title = {Constrained Markov Decision Processes},
	abstract = {book turned out to be a rich and interesting constrained control problem in itself. The objectives were not always easy to quantify and many evident constraints came out, such as time and page limitations. With the help of the theory developed here as well as the warm support of my wife, {TANIA}, we were nally able to meet the constraints and present a solution, that we hope you will enjoy reading. Eitan Altman, August 1998 Contents 1 Introduction 1 1.1 Examples of constrained dynamic control problems 1 1.2 On solution approaches for {CMDPs} with expected costs 3 1.3 Other types of {CMDPs} 5 1.4 Cost criteria and assumptions 7 1.5 The convex analytical approach and occupation measures 8 1.6 Linear Programming and Lagrangian approach for {CMDPs} 10 1.7 About the methodology 12 1.8 The structure of the book 17 I Part One: Finite {MDPs} 19 2 Markov decision processes 21 2.2 Cost criteria and the constrained problem 23 2.3 Some notation 24 3.2 Dynamic programming and dual {LP}: the unconstrained c},
	author = {Altman, Eitan and Asingleutility, Inmanysituationsintheoptimizationofdynamicsystems},
	date = {1999},
	file = {Citeseer - Full Text PDF:/home/nle6013/Zotero/storage/LU8BKWYS/Altman and Asingleutility - 1999 - Constrained Markov Decision Processes.pdf:application/pdf;Citeseer - Snapshot:/home/nle6013/Zotero/storage/CEGCI2EE/summary.html:text/html}
}

@inproceedings{jansen_exploring_2003,
	title = {Exploring the Explorative Advantage of the Cooperative Coevolutionary (1+1) {EA}},
	isbn = {978-3-540-40602-0},
	url = {https://link.springer.com/chapter/10.1007/3-540-45105-6_37},
	doi = {10.1007/3-540-45105-6_37},
	series = {Lecture Notes in Computer Science},
	abstract = {Using a well-known cooperative coevolutionary function optimization framework, a very simple cooperative coevolutionary (1+1) {EA} is defined. This algorithm is investigated in the context of expected optimization time. The focus is on the impact the cooperative coevolutionary approach has and on the possible advantage it may have over more traditional evolutionary approaches. Therefore, a systematic comparison between the expected optimization times of this coevolutionary algorithm and the ordinary (1+1) {EA} is presented. The main result is that separability of the objective function alone is is not sufficient to make the cooperative coevolutionary approach beneficial. By presenting a clear structured example function and analyzing the algorithms’ performance, it is shown that the cooperative coevolutionary approach comes with new explorative possibilities. This can lead to an immense speed-up of the optimization.},
	eventtitle = {Genetic and Evolutionary Computation Conference},
	pages = {310--321},
	booktitle = {Genetic and Evolutionary Computation — {GECCO} 2003},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Jansen, Thomas and Wiegand, R. Paul},
	urldate = {2018-07-31},
	date = {2003-07-12},
	langid = {english},
	file = {Snapshot:/home/nle6013/Zotero/storage/8WB26RWV/10.html:text/html}
}


@book{lewis_convention:_1969,
	title = {Convention: A Philosophical Study},
	shorttitle = {Convention},
	publisher = {Wiley-Blackwell},
	author = {Lewis, David},
	date = {1969}
}

@article{kaelbling_reinforcement_1996,
	title = {Reinforcement Learning: A Survey},
	url = {http://arxiv.org/abs/cs/9605103},
	shorttitle = {Reinforcement Learning},
	abstract = {This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.},
	journaltitle = {{arXiv}:cs/9605103},
	author = {Kaelbling, L. P. and Littman, M. L. and Moore, A. W.},
	urldate = {2018-08-07},
	date = {1996-04-30},
	eprinttype = {arxiv},
	eprint = {cs/9605103},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv\:cs/9605103 PDF:/home/nle6013/Zotero/storage/WBSPSITR/Kaelbling et al. - 1996 - Reinforcement Learning A Survey.pdf:application/pdf}
}

@online{noauthor_master_nodate,
	title = {Master - {ROS} Wiki},
	url = {http://wiki.ros.org/Master},
	urldate = {2018-08-01},
	file = {Master - ROS Wiki:/home/nle6013/Zotero/storage/G62ZMC79/Master.html:text/html}
}

@online{noauthor_phantomx_nodate,
	title = {{PhantomX} Pincher Robot Arm Kit},
	url = {https://www.trossenrobotics.com/p/PhantomX-Pincher-Robot-Arm.aspx},
	urldate = {2018-08-01},
	file = {PhantomX Pincher Robot Arm Kit:/home/nle6013/Zotero/storage/87D677AQ/PhantomX-Pincher-Robot-Arm.html:text/html}
}

@online{noauthor_robot_2017,
	title = {Robot Operating System},
	url = {https://robotics.oregonstate.edu/robot-operating-system},
	titleaddon = {Collaborative Robotics and Intelligent Systems ({CoRIS}) Institute},
	urldate = {2018-08-01},
	date = {2017-02-15},
	langid = {english},
	file = {Snapshot:/home/nle6013/Zotero/storage/HXM8BUXX/robot-operating-system.html:text/html}
}