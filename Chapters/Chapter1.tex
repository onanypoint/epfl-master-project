\chapter{Introduction}
\lhead{\emph{Introduction}} 
Reinforcement learning has seen a lot of improvement since its first introduction and it has now been used to help solve many tasks thought before as unsolvable. For example, reinforcement learning has been used to beat human players at Atari games \cite{mnih_playing_2013} or master the game of GO \cite{silver_mastering_2016}. Moreover, reinforcement learning by solving the task of sequential decision making in an unknown environment is particularly well suited to solve tasks where it is difficult to obtain training data (robotics, telecommunication, etc.). 

% http://www.scholarpedia.org/article/Reinforcement_learning#Challenges_and_extensions_to_RL

Despite the usefulness of reinforcement learning in many fields, the method can lead to several problems. In an ideal world, any machine learning method should be able to cope with noisy input. Moreover, it should be general enough and scalable. In order to be able to learn, an agent requires feedback about the quality of the action taken. Unfortunately in most of the real-world scenarios, the feedback might be temporally delayed and as a result only impact the preceding states and actions weakly. For example, a robot might have to take several actions before relevant events happen. Several reinforcement learning algorithms assume the environment to be Markovian and that it is irrelevant how it has reached a certain state. Reinforcement learning agents might not know exactly in what state they are or will be after performing an action. Consequently, the preceding assumption is broken. Finally, reinforcement learning can only cope with dynamics that change slowly. This is a fundamental problem. Convergence of the learning might not be achievable if the world changes too fast \cite{kaelbling_reinforcement_1996}. Setting up a reinforcement learning problem might reveal itself to be difficult. Whether it is a real-world problem or a simulated one, appropriate state and action spaces need to be determined. In order to cover all possible situations, the state space as well as the available actions might be large. Exploring all possible pairs can then lead to a combinatorial explosion \cite{busoniu_comprehensive_2008, busoniu_multi-agent_2010}. The situation worsens when multiple agents are involved. The problem becomes exponential in the number of agents \cite{l._leottau_decentralized_2017}.

In order to cope with the curse of dimensionality, several solutions have been proposed such as generalising value functions. This is either done using function approximation methods or by interpolation of the searchable value-space. When the "action dimensions are able to operate in parallel and their individual information and resources can be managed separately", the task can be decentralised \cite{l._leottau_decentralized_2017}. It has been shown that decentralised systems are able to achieve similar learning times and comparable or slightly lower performance. To compensate the lack of direct coordination between the agents which can lead to lower performance, inter-agent communication can be used. Communication can help increase the team performance by mitigating two sources of uncertainty. It can help agents by increasing their knowledge about the state of the system as well as the intention of the other agents \cite{t._j._spaan_decentralized_2006}. The following explores this specific approach in a robotic transport system. In particular, the usage of the learned communication protocols is examined in-depth. 

\section{Contributions}

This Master project explores the application of distributed reinforcement learning in a coordination problem involving robotic arms. It explores inter-agent communication in a $Dec$-$POMDP$ environment to enable learning in a fully decentralised fashion. The communication protocols are also examined in order to to gain insight in the influence of the hyperparameters on the solutions. 

\section{Limits}

In order to study the behaviour of the agents, several assumptions had to be made. Even though the underlying problem is linked to the physical world, only simulated experiments are used. The problem is sufficiently simple to obtain good understanding of the underlying process and know which state information is relevant. This might not always be possible. This work relies on tabular Q-Learning. It enables faster learning and a clear view of the learning process. Unfortunately, this learning algorithm does not scale to more complex problems. Also the simulated environment makes assumption about the available interaction between the entities. The simulated environment look pass many problems that could arise in a real-world setting.

\section{Outline}
The work is organised as follows. In \autoref{chap:background}, the theoretical background linked to reinforcement learning and the Markov decision process framework are introduced. Furthermore, the necessary extensions needed by the transition from single agent to a multi-agent system as well as the related works are presented. The \autoref{chap:experiments} presents the experiment setting used in \autoref{chap:discussion}. The latter presents, examines and discusses the results. In particular this chapter describes the influence of hyperparameters on the quality of the learned communication. Finally, \autoref{chap:conclusion} discusses future work and concludes.