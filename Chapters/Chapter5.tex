\chapter{Future Work and Conclusion}
\lhead{\emph{Conclusion}} 
\label{chap:conclusion}

\section{Future research}

Several lines of research arising from this work should be pursued. Firstly, investigating more complex problems and learning methods might prove important. The current work relies solely on tabular Q-Learning and might be too limited for learning more complex tasks. Also, future research is needed to investigate the possibilities of using delayed instead of instantaneous communication. Finally, in the current work, the agents have no implicit incentive to be fast. It is thus possible to assume that the influence of the discount rate on the quality of the protocols might change when the incentive exists. This assumption should be addressed in future studies.

\section{Conclusion}

This work demonstrates that a simulated transport problem involving robotic arms can be solved using decentralised learners and inter-agent communication. Being able to solve a reinforcement learning problem in a decentralised manner can help when the problem becomes intractable by a centralised scheme. Furthermore, this work shows that increasing the vocabulary size available to a communicating agent improves the quality of the learned solution. On a higher level, the cost of communication can be used to influence the grounding of communication used by the agents. When a problem can be aliased to a referential game, a low discount rate can be used to increase the chance of converging to an ideal protocol.


